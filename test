def select_words(s, n):
    """Given a string s and a natural number n, you have been tasked to implement
    a function that returns a list of all words from string s that contain exactly
    n consonants, in order these words appear in the string s.
    If the string s is empty then the function should return an empty list.
    Note: you may assume the input string contains only letters and spaces.
    Examples:
    select_words("Mary had a little lamb", 4) ==> ["little"]
    select_words("Mary had a little lamb", 3) ==> ["Mary", "lamb"]
    select_words("simple white space", 2) ==> []
    select_words("Hello world", 4) ==> ["world"]
    select_words("Uncle sam", 3) ==> ["Uncle"]
    """


def gen_instruct_prompt(prompt: str, template: str = "") -> str:
    """
    Converts the human-eval prompts for completion tasks
    to an instruction prompt to be fed into our instruct-tuned models.
    example:
    from typing import List def all_prefixes(string: str) -> List[str]: \"\"\" {{PROBLEM description}} \"\"\"
    becomes:
    Please complete the following Python code without providing any additional tasks such as testing or explanations
    ### Instruction: Write a Python function 'all_prefixes(string: str) -> List[str]' to solve the following problem:
    :param prompt: the human-eval prompt
    :return: the instruction prompt
    """
    seperator = '"""'  # most of the prompts have this seperator
    if prompt.count(seperator) == 0:
        seperator = "'''"  # some cases use single quotes
    # split the prompt by the triple quotes
    function_signature, description, _ = prompt.split(seperator, 2)
    # get the pure function signature, starts with "def"
    found_def = function_signature.find("def") != -1
    # HumanEval/64 testcase has an extra """ before def so it messes up the split above"
    # if it's a wrong split, try to iterate through the split and find the one that starts with def
    if not found_def:
        splitted_prompt = prompt.split(seperator)
        signeture_indx = -1
        for i, element in enumerate(splitted_prompt):
            if (element.strip().find("def")) != -1:
                signeture_indx = i
                found_def = True
                break
        if signeture_indx == -1:
            found_def = False  # not found at all

        function_signature = splitted_prompt[signeture_indx] if found_def else ""
        description = splitted_prompt[signeture_indx + 1] if found_def else description

    function_signature = (
        function_signature[function_signature.find("def") : -2].strip()
        if found_def
        else ""
    )
    description = description.strip()
    # if the prompt doesn't start with "def" right away, it means it had a meanful code before it like imports or specifications
    # so we need to add them to the prompt somehow, we add them as ### Input: using the Alpaca instruction prompt
    pre_def = ""
    if prompt.strip().find("def") != 0:
        pre_def = prompt[: prompt.find("def")].strip()

    # everything is ready now.
    # notice we updated the system message to avoid additional boilerplate code
    if template == "alpaca" or template == "phi":
        if pre_def:
            description += f"\n\n### Input:\n start the code with {pre_def}"

        system_msg = "Below is an instruction that describes a task. Write a response that appropriately completes the request.\nPlease complete the following Python code without providing any additional tasks such as testing or explanations\n\n"
        prompt = (
            system_msg
            + f"### Instruction: Write a Python function '{function_signature}' to solve the following problem:\n{description}\n\n### Output:\n"
        )
    elif template == "mistral":
        if pre_def:
            description += f"\nINPUTS: start the code with '{pre_def}'"  # fixed some cases that imports not written in the generated code

        system_msg = "Below is an instruction that describes a programming task. Write a response code that appropriately completes the request.\nPlease complete the following Python code without providing any additional tasks such as testing or explanations\n"
        prompt = f"<s>[INST] {system_msg}\nWrite a Python function '{function_signature}' to solve the following problem:\n{description} [/INST]"
    else:
        if pre_def:
            description += f"\nInputs:\n{pre_def}"
        system_msg = "Below is an instruction that describes a task. Write a response that appropriately completes the request.\nPlease complete the following Python code without providing any additional tasks such as testing or explanations\n\n"
        prompt = f"Write a Python function '{function_signature}' to solve the following problem:\n{description}"

    return prompt


def generate_prompt(text: str, *, prompt_template: str = ""):
    if prompt_template:
        try:
            prompt = prompt_template.format(text)
        except KeyError:
            # if the prompt template is not valid, use the original prompt
            print(
                "Invalid prompt template, using original prompt. Make sure to include {} in the template."
            )
            return text
        return prompt

    if "codegen" in self.model_name.lower():
        # style "alpaca":
        system_msg = f"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n"
        return system_msg + f"### Instruction: {text}\n\n### Output:\n"

    if "mistral" in self.model_name.lower():
        system_msg = "Below is an instruction that describes a programming task. Write a response code that appropriately completes the request.\n"
        return f"<s>[INST] {system_msg}\n{text} [/INST]"

    return text


def run(self, prompt: str, **kwargs):
    input_ids = self.tokenizer(
        prompt, return_tensors="pt", add_special_tokens=True
    ).input_ids
    if not self.load_8bit:
        input_ids = input_ids.to(self.device)

    generated_ids = self.model.generate(
        input_ids,
        max_new_tokens=self.max_output_length,
        pad_token_id=self.tokenizer.eos_token_id,  # avoid warning
        **kwargs,
    )

    output = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    return output


def respond(message, chat_history, additional_inputs):
    if message == "" or None:
        return "Please enter a prompt"

    model_name = additional_inputs
    modelEval = model[model_name]
    template = model_template[model_name]

    response = modelEval.run(message, verbose=True, pure_mode=False)
    response = filter_code(response, template=template)
    return f"Here's an example code:\n\n```python\n{response}\n```"
