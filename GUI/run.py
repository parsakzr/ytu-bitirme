# -*- coding: utf-8 -*-
"""Run eval model and Gradio

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bbMjPdSmI1hm8gdWBmvzbg4kpm2QwEKM
"""

# import locale
# def getpreferredencoding(do_setlocale = True):
#     return "UTF-8"
# locale.getpreferredencoding = getpreferredencoding

from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import BitsAndBytesConfig
import torch
from codellm.model import EvalModel
import gradio as gr

"""Custom code to handle Model inputs/outputs"""


def generate_prompt(text: str, model_name, prompt_template: str = ""):
    if prompt_template:
        try:
            prompt = prompt_template.format(text)
        except KeyError:
            # if the prompt template is not valid, use the original prompt
            print(
                "Invalid prompt template, using original prompt. Make sure to include {} in the template."
            )
            return text
        return prompt

    if "codegen" in model_name.lower() or "phi" in model_name.lower():
        # style "alpaca":
        system_msg = f"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n"
        return (
            system_msg
            + f"### Instruction: Write a high quality Python code to solve the following problem:\n{text}\n\n### Response:\n"
        )

    if "mistral" in model_name.lower():
        system_msg = "Below is an instruction that describes a programming task. Write a response code that appropriately completes the request.\n"
        return f"<s>[INST] {system_msg}\nWrite a high quality Python code to solve the following problem:\n{text} [/INST]"

    return text


def filter_code(completion: str, prompt: str = None, template: str = "") -> str:
    try:
        code = completion
        if prompt is not None:
            # remove the prompt, since it's a completion model
            code = code.replace(prompt, "")

        if template == "alpaca":
            ## Remove boilerplate for the function, reused pure_mode in generation_pipeline
            # select the text between the two '''
            code = code.split("'''")[1]
            # remove the first line (which is the language)
            code = "\n".join(code.split("\n")[1:])

        if template == "phi":
            ## Remove boilerplate for the function, reused pure_mode in generation_pipeline
            # select the text between the two '''
            code = code.split("```")[1]
            # remove the first line (which is the language)
            code = "\n".join(code.split("\n")[1:])

        if template == "mistral":
            # get the code inside [CODE] ... [/CODE]
            code = code.split("[CODE]")[1]
            code = code.split("[/CODE]")[0]

        ## The program tends to overwrite, we only take the first function
        code = code.lstrip("\n")
        return code.split("\n\n")[0]
    except Exception as e:
        print(e)
        return code
    finally:
        return code


def respond(message, chat_history, additional_inputs):
    if message == "" or None:
        return "Please enter a prompt"

    model_name = additional_inputs
    modelEval = model[model_name]
    template = model_template[model_name]

    prompt = generate_prompt(
        message,
        modelEval.model_name,
    )
    response = modelEval.run(message)
    response = filter_code(response, template=template)
    return f"Here's an example code:\n\n```python\n{response}\n```"


def run():
    """
    Getting all the models (4bit quantized)
    """

    model_ids = {
        "codegen": "parsak/codegen-350M-mono-lora-instruction",
        "mistral": "parsak/mistral-code-7b-instruct",
        "phi": "parsak/phi-2-code-instruct",
    }

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
    )

    mistralModel = AutoModelForCausalLM.from_pretrained(
        model_ids["mistral"], quantization_config=bnb_config, device_map={"": 0}
    )
    # tokenizer
    mistralTokenizer = AutoTokenizer.from_pretrained(model_ids["mistral"])
    mistralTokenizer.pad_token = mistralTokenizer.eos_token
    mistralTokenizer.padding_side = "right"

    mistralEvalModel = EvalModel()
    mistralEvalModel.load_from(mistralModel, mistralTokenizer)
    mistralEvalModel.model_name = model_ids["mistral"].split("/")[-1]

    # CodeGen
    codegenModel = AutoModelForCausalLM.from_pretrained(
        model_ids["codegen"], quantization_config=bnb_config, device_map={"": 0}
    )
    # tokenizer
    codegenTokenizer = AutoTokenizer.from_pretrained(model_ids["codegen"])
    codegenTokenizer.pad_token = codegenTokenizer.eos_token
    codegenTokenizer.padding_side = "right"

    codegenEvalModel = EvalModel()
    codegenEvalModel.load_from(codegenModel, codegenTokenizer)
    codegenEvalModel.model_name = model_ids["codegen"].split("/")[-1]

    # Phi-2-Code
    phiModel = AutoModelForCausalLM.from_pretrained(
        model_ids["phi"],
        quantization_config=bnb_config,
        device_map={"": 0},
        trust_remote_code=True,
    )

    # tokenizer
    phiTokenizer = AutoTokenizer.from_pretrained(model_ids["phi"])
    phiTokenizer.pad_token = phiTokenizer.eos_token
    phiTokenizer.padding_side = "right"

    phiEvalModel = EvalModel()
    phiEvalModel.load_from(phiModel, phiTokenizer)
    phiEvalModel.model_name = model_ids["phi"].split("/")[-1]

    print(mistralEvalModel.model_name)
    print(codegenEvalModel.model_name)
    print(phiEvalModel.model_name)

    # models organized in a dictionary
    model = {
        mistralEvalModel.model_name: mistralEvalModel,
        codegenEvalModel.model_name: codegenEvalModel,
        phiEvalModel.model_name: phiEvalModel,
    }
    model_template = {
        codegenEvalModel.model_name: "alpaca",
        mistralEvalModel.model_name: "mistral",
        phiEvalModel.model_name: "phi",
    }

    choices = list(model.keys())
    dropdown = gr.Dropdown(
        label="Models", choices=choices, value=phiEvalModel.model_name
    )

    # create the interface
    interface = gr.ChatInterface(
        respond,
        retry_btn=gr.Button(value="Retry"),
        undo_btn=None,
        clear_btn=gr.Button(value="Clear"),
        additional_inputs=[dropdown],
    )

    # run the interface
    interface.launch(debug=True)


if __name__ == "__main__":
    run()
